\documentclass{statsoc} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage{amsmath,amssymb}
\usepackage{bm}
\usepackage{natbib}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{numdef}
\usepackage{multibib}

\input{notation.tex}

\newcites{supp}{Supplementary References}


%%% TO DO
%% Add detail to application section
%% fill in missing citations
%% Appendix
%%   clean up/fix notation for math
%%   add in full simulation results
%%   small interaction simulation
%%   look for what else I promised and either put it in or delete promise
%% proofread

\title[GEEPERS]{GEEPERs: Principal Stratification using Principal Scores and Stacked Estimating Equations}

\author[Sales and Vanacore]{Adam C Sales}

\author{Kirk Vanacore}
\address{Worcester Polytechnic Institute,
Worcester, MA,
USA.}
\email{asales@wpi.edu}


\begin{document}
\maketitle

\begin{abstract}
Principal stratification is a framework for making sense of causal effects conditioned on variables that themselves may have been affected by treatment. Most principal stratification estimators rely on strong structural or modeling assumptions, and many require advanced statistical training to fit and to check. In this paper, we introduce a new M-estimation principal effect estimator for one-way noncompliance based on a binary indicator. Estimates may be computed using conventional regressions (though the standard errors require a specialized sandwich formula) and do not rely on distributional assumptions. We illustrate the new technique in an analysis of student log data from a recent educational technology field experiment.  
\end{abstract}
\keywords{Causal Inference; Principal Stratification; Educational Technology}

\section{Introduction}

Randomized experiments easily admit subgroup analysis---estimation of treatment effects for a subset of the study population---when those subgroups are defined at baseline, and not themselves a function of treatment assignment.
%A simple, popular approach to estimating subgroup effects is to use ordinary least squares (OLS) regression, regressing the outcome on a treatment indicator interacted with an indicator for subgroup membership.
However, when subgroup membership is a function of a post-treatment variable, then it may itself be affected by the treatment.
Contrasts between treatment and control conditions within those subgroups may not have a causal interpretation at all---and when they do, the same techniques that work for subgroups defined at baseline %(such as OLS with interactions) 
will often produce inconsistent estimates.

Principal stratification \citep{frangakis} is a broad framework for defining subgroup effects for post-treatment subgroups. It relies on the notion of \emph{potential} subgroups called principal strata: groups of subjects defined based on the values a post-treatment variable would potentially take if the subject were assigned to one treatment condition or another.
Since principal strata membership is based on subjects' responses to both factual and counter-factual treatment assignments, it is in general unobserved.
Nonetheless, researchers have devised estimators of principal effects, as average effects within principal strata are called, to address a broad range of applied statistical problems, including non-compliance in randomized experiments \citep{air}, the evaluation of surrogate outcomes \citep{li2010bayesian}, the investigation of causal mechanisms \citep{lidsayPage}, and sample attrition \citep{zhangRubin, ding2011}.

Because principal strata membership is generally unobserved, principal effects are not always identified, and when they are, estimation can still be difficult.
Typically, principal effects are identified and estimated based on strong structural assumptions \citep[e.g.][]{air} and/or parametric modeling \citep[e.g.][]{imbens1997bayesian}.
Parametric models for principal effects are often hard to specify and hard or impossible to test.
Even when they are well-specified they can yield estimates that are severely biased and misleading \citep{griffin2008application,feller2016principal}.
Another route to identification and estimation of principal effects leads through auxiliary data, including baseline covariates or secondary outcomes \citep{mattei2013exploiting}.
Of particular importance is the ``principal score,'' \citep{jo,dingLu,feller2017principal}, or the probability that a subject belongs to a particular principal stratum, as a function of pre-treatment covariates.
Under a strong, untestable assumption called ``principal ignorability,''  principal scores can be transformed into weights to compute form an unbiased estimate of principal effects.

In a separate, if related, line of research, \citet{jo2002}, \citet{ding2011}, \citet{jiangDing2021}, and others have shown that, under certain circumstances, researchers can use baseline covariates to identify and estimate principal effects without relying on principal ignorability.
This paper expands on those identification results by presenting a new set of principal effect estimators, and associated standard error estimates, based on a general estimating equations (or M-estimation or method-of-moments) \citep{stefanskiBoos}.
It turns out that, under certain general conditions, principal effects can be estimated with a simple, two-step procedure: first, estimate principal scores with logistic regression or another M-estimator; then, impute unknown principal subgroup membership indicators with principal scores and estimate principal effects using ordinary least squares (OLS) regression with interactions. %, just as in the pre-treatment subgroup case.
We will refer to the estimators presented here as ``GEEPERs'': generalized estimating equations for principal effects using regressions.
GEEPERs estimates rely on correct specification of regression functions, but unlike common fully-parametric methods, they make no assumptions about the shape of the distribution of regression errors.
Unlike principal score weighting, GEEPERs does not assume principal ignorability, and admits to sandwich-style standard error estimates, which we provide in the appendix.

This paper focuses on a special case of principal stratification---one-way, binary non-compliance---that is, there are only two principal strata, and principal stratum membership is observable in one treatment arm, but not in the other.
This scenario would occur if, say, subjects in the control arm of a study had no access to the treatment, and not all subjects assigned to the treatment arm actually received the full treatment.
While such a setup may represent the simplest and easiest principal stratification problem, we believe it is a common scenario.
Moreover, we believe that the methods developed here can extend to more complex scenarios, which we leave to future work.

The following section gives a formal introduction to principal stratification and briefly describes two common estimation techniques, parametric mixture modeling and principal score weighting.
Next, Section \ref{sec:geepers} develops GEEPERs estimation, and gives conditions for its consistency.
Section \ref{sec:simulation} describes a simulation study examining the operating characteristics of GEEPERs estimates and comparing them to estimates from parametric mixture modeling and principal score weighting.
Under ideal circumstances, estimates from parametric mixture models and weighting estimators are somewhat more efficient than GEEPERs, but when assumptions are violated GEEPERs easily out-performs the other two methods.
Section \ref{sec:fh2t} illustrates GEEPERs in analyzing data from a recent large-scale field trial of educational software, and Section \ref{sec:discussion} concludes.



\section{Background}
In a randomized experiment with two conditions, let $Z_i=0$ if subject $i$, $i=1,\dots,n$ is assigned to the treatment condition, and let $Z_i=0$ otherwise.
If interest is in the effect of $Z$ on an outcome $Y$, under the ``stable unit treatment value assumption'' \citep{air} of no interference between units and no hidden versions of the treatment, define potential outcomes \citep{splawa1990application} $\yti$ as the value of $Y$ that $i$ would exhibit if $Z_i=1$, and let $\yci$ be the value if $Z_i=0$. Then the observed value of $Y$ satisfies $Y_i=Z_i\yti+(1-Z_i)\yci$, and the treatment effect of $Z$ on $Y$ for subject $i$ may be defined as $\tau_i=\yti-\yci$. The average treatment effect (ATE) is $\EE[\tau]$. %

Let $S_i\in\{0,1\}$ be a measurement on subject $i$ taken subsequent to treatment assignment, so that $S$ may be affected by $Z$.
Then $S$ itself has potential values $\sti$ and $\sci$, which $i$ would exibit if $Z_i=1$ or $Z_i=0$, respectively.
Assume ``strong monotonicity'' \citep[c.f.][]{dingLu} (also referred to as ``one-way noncompliance''):%\citealt{wing2017can}):
\begin{ass}[Strong Monotonicity]\label{ass:sm}
\begin{equation*}%\label{eq:strongMonotonicity}
  \sci=0\text{ for all }i
\end{equation*}
\end{ass}
Like potential outcomes $\yti$ and $\yci$, $\sci$ and $\sti$ are defined for all subjects, though $\sci$ is only observed when $Z_i=0$ and $\sti$ is only observed when $Z_i=1$. Under strong monotonicity, since $S$ is binary, every subject belongs to one of two ``principal strata,'' $\sti=1$ or $\sti=0$.
The average treatment effect in each principal stratum, $\eff1=\EE[\tau|\st=1]$ or $\eff1=\EE[\tau|\st=0]$, is called a principal effect. %
The challenge in estimating principal effects is due to the fact that strata membership $\sti$ is only observed when $Z_i=1$, in which case $\sti=S_i$, the observed value of $S$.

\subsection{Estimating Principal Effects}
In estimating principal effects for a binary $S$, it will be useful to define four conditional means. For $s\in\{0,1\}$, let 
\begin{equation}\label{eq:mus}
\mucs=\EE[Y_C|\st=s] \mbox{ and }\muts=\EE[Y_T|\st=s]%\mbox{ for }s=0,1
  % \begin{split}
  %   \muc0&=\EE[Y_C|\st=0]\\
  %   \muc1&=\EE[Y_C|\st=1]\\
  %   \mut0&=\EE[Y_T|\st=0]\\
  %   \mut1&=\EE[Y_T|\st=1]
  % \end{split}
\end{equation}
Then, since
\begin{equation*}
  \effs=\muts-\mucs
  % \begin{split}
  %   \eff0&=\mut0-\muc0\\
  %   \eff1&=\mut1-\muc1
  % \end{split}
\end{equation*}
estimating principal effects requires estimating the four conditional means.

We will focus on estimation in completely randomized experiments, that is, we will assume
\begin{ass}[Randomization]\label{ass:rand}
\begin{equation*}%\label{eq:randomization}
  \yci,\yti,\sti, \bm{x}_i \independent Z_i
\end{equation*}
\end{ass}
where $\bm{x}_i$ is a vector of pre-treatment covariates.

Under Assumptions \ref{ass:sm} and \ref{ass:rand}, conditional means $\mut0$ and $\mut1$ are non-parametrically identified.
In particular, the means of observed outcomes for subjects with $Z=1$ and $S=0$ or $S=1$ are unbiased for $\mut0$ and $\mut1$, respectively, since when $Z_i=1$, $Y_i=\yti$ and $S_i=\sti$.
In contrast, $\muc0$ and $\muc1$ are not fully identified without further assumptions because $\sti$ and $\yci$ are never observed simultaneously (though they may be nonparametrically bounded; see, e.g., \citealt{bounding}).
% When $S$ is a measure of compliance, and $\tau_i=0$ whenever $\sti=0$ (the ``exclusion restriction''), $\muc1$---and hence $\eff1$---may be estimated with instrumental variables methods \citep{air}.
% In this case, $\eff1$ is often called the ``local average treatment effect'' or the average effect of the treatment on the treated.
We will briefly review two approaches to estimating $\muc0$ and $\muc1$% that do not assume the exclusion restriction
---normal mixture modeling and weighting---with some comments in-between on the role for covariates.

\subsubsection{Normal Mixture Modeling}
The normal mixture modeling approach to estimating $\muc0$ and $\muc1$ \citep{imbens1997bayesian} assumes that, conditional on $\st$, $Y_C$ is normally distributed.
Then the probability density of $Y_C$ may be written as
\begin{equation}\label{eq:mixtureModel}
  f_{Y_C}(y)=Pr(\st=0)\phi\left(\frac{y-\muc0}{\sigma_C^0}\right)+Pr(\st=1)\phi\left(\frac{y-\muc1}{\sigma_C^1}\right)
\end{equation}
where $\phi(\cdot)$ is the standard normal density function, and $\sigma_C^0>0$ and $\sigma_C^1>0$ are standard deviations, which are sometimes assumed equal.
The probabilities $Pr(\st=0)$ and $Pr(\st=1)$ may be estimated first using data from the treatment group, since Assumption \ref{ass:rand} implies that $Pr(\st=1|Z=1)=Pr(\st=1)$.
Given these estimates and \eqref{eq:mixtureModel}, parameters $\muc0$ and $\muc1$ may be estimated using maximum likelihood, method of moments, or Bayesian techniques.

This approach has a number of setbacks.
First, the conditional normality assumption is untestable and limiting.
Unfortunately, even when it holds, mixture model estimates can be unreliable and, in some cases, severely biased \citep{griffin2008application,feller2017principal}.

\subsection{Principal Scores}
The principal score \citep[e.g.][]{jo} is defined as
\begin{equation}\label{eq:pscore}
  \ppi=Pr(\sti=1|\bxsi)
\end{equation}
where $\bxsi\subseteq\bm{x}_i$, the set of covariates used to model $S$.
Note that under randomization, $Pr(\sti=1|\bxsi,Z_i)=Pr(\sti=1|\bxsi)$, so a model for principal scores can be estimated using data from the treatment group and extrapolated to the control group.
Assume a model for principal scores:
\begin{equation}\label{eq:prinScoreMod}
  \ppip{\bm{\alpha}}=f(\bxsi;\bm{\alpha})
\end{equation}
with parameter vector $\bm{\alpha}$. An analyst may estimate $\bm{\alpha}$ by fitting model \eqref{eq:prinScoreMod} using observed $\bm{S}$ and $\bxs$ for subjects with $Z=1$, and then compute estimated principal scores $\ppip{\bm{\hat{\alpha}}}$ for subjects with $Z_i=1$.

Going forward, we will occasionally suppress dependence on $\bm{\alpha}$, and write $\ppip{\bm{\alpha}}=\ppi$.

In order for principal scores to be useful, they need to vary with $\bm{x}$. Otherwise, $\bm{x}$ is uninformative about $\st$. In particular, we will assume the following:
\begin{ass}[Variable Principal Scores]\label{ass:vps}
 $\pp$ takes at least 3 distinct values
\end{ass}
%In some estimators we will discuss below, two distinct values suffice. 
For a broader discussion, see \citet{ding2011,jiangDing2021}.

Principal scores can potentially improve inference in a finite mixture model, by re-writing \eqref{eq:mixtureModel} as
\begin{equation}\label{eq:mixtureModelPS}
  f_{Y_C}(y)=(1-\ppip{\bm{\alpha}})\phi\left(\frac{y-\muc0}{\sigma_C^0}\right)+\ppip{\bm{\alpha}}\phi\left(\frac{y-\muc1}{\sigma_C^1}\right)
\end{equation}
%Ideally, a statistician will estimate parameters $\muc0$, $\muc1$, $\mut0$, $\mut1$, $\sigma^0_C$, $\sigma^1_C$, and $\bm{\alpha}$ simultaneously, using maximum likelihood or Bayesian techniques.

Alternatively, an analyst may assume ``principal ignorability'' \citep{jo,dingLu}:
\begin{ass}[Principal Ignorability]\label{ass:PI}
\begin{equation}\label{eq:pi}
  Y_{C}\independent \st |\bxs
\end{equation}
\end{ass}
or a somewhat weaker version, $\EE[Y_C|\st,\bxs]=\EE[Y_C|\bxs]$ \citep{feller2017principal}.
That is, the principal stratum is unrelated to control potential outcomes conditional on covariates.
Principal ignorability \eqref{eq:pi} is reminiscent of ignorability assumptions typical in observational studies. % \citep[e.g.][]{rosenbaum2010design}.
% Both require a rich set of covariates and background knowledge of the determinants of $S$; 
In particular, an unobserved covariate $u$ that is correlated with both $\st$ and $Y_C$ would invalidate \eqref{eq:pi}.

Under principal ignorability, $\muc0$ and $\muc1$ can be estimated via weighting:
\begin{equation}\label{eq:psw}
  \hat{\muc0}_w=\frac{\sum_{i: Z_i=0} Y_i(1-\pp)}{\sum_{i:Z_i=0}1-\pp}\text{ and } \hat{\muc1}_w=\frac{\sum_{i: Z_i=0} Y_i(\pp)}{\sum_{i:Z_i=0}\pp}
\end{equation}

Principal score weighting does not require any distributional assumptions and (after estimating principal scores) is very easy to implement.
On the other hand, the principal ignorability assumption is strong and restrictive.

\citet{feller2017principal} recommends the case-resampling bootstrap to estimate the sampling variances of $\hat{\mu}_{Cw}^0$, $\hat{\mu}_{Cw}^1$, and their associated principal effect estimators.


\section{GEEPERs}\label{sec:geepers}

The approach we introduce here incorporates principal scores into an M-Estimator for the mixture model \eqref{eq:mixtureModel}.
Like principal score weighting, our M-estimator does not require distributional assumptions, but instead requires a conditional independence assumption.
We will begin by describing a stronger-than-necessary conditional independence assumption in order to build intuition; in \S~\ref{sec:regression} we will present a considerably weaker alternative.

\subsection{Building Intuition: Covariate Ignorability}\label{sec:ci}

We begin by introducing an assumption that we term ``Covariate Ignorability,'' or CI. It is a slightly weaker version of \citet{jiangDing2021}'s ``auxiliary independence'' assumption.
\begin{ass}[Covariate Ignorability]\label{ass:ci}
\begin{equation}\label{eq:assumption}
\EE[\yci|\bxsi,\sti]=\EE[\yci|\sti]=\muc0\text{ or }\muc1
\end{equation}
\end{ass}
i.e. $Y_C$ is mean-independent of $\bxs$ conditional on $\st$.
Under CI, covariates are not informative of the mean of $Y_C$ within principal strata.
As stated above, CI will rarely be plausible.
However, in some circumstances researchers can identify a subset of observed covariates that satisfy CI, and use those covariates in estimation.

It turns out that under strong monotonicity, randomization, and CI, and given a set of principal scores, the two principal effects $\eff0$ and $\eff1$ can be estimated by a simple OLS regression.
To see how, we'll %first
derive the estimating equations for $\muc0$, $\muc1$, $\mut0$, and $\mut1$, and show that they are equivalent to estimating equations for a model fit by OLS. % assuming that principal scores are known exactly, and then generalize to the case in which they are estimated using data from the treatment group.
The details of the calculations and the proofs are in the appendix.

The argument stems from a set of expressions for expectations, summarized in the following lemma:
\begin{lemma}\label{lemma:expectation}
Let \begin{equation}\label{eq:estEq0}
\tilde{\Psi}_i=\begin{pmatrix}
    (1-Z_i)\left[Y_i-\muc0-\ppi(\muc1-\muc0)\right]\\
    (1-Z_i)\left[\ppi Y_i-\ppi\muc0-\ppi^2(\muc1-\muc0)\right]\\
    Z_i\left[Y_i-\mut0-\sti (\mut1-\mut0)\right]\\
    Z_i\left[\sti Y_i -\sti\mut0-\sti^2(\mut1-\mut0)\right]
  \end{pmatrix}
\end{equation}
Then under Assumptions \ref{ass:sm}, \ref{ass:rand}, \ref{ass:vps}, and \ref{ass:ci}, $\EE[\Psi_i]=\bm{0}$.
\end{lemma}

Lemma \ref{lemma:expectation} implies a set of estimating equations $\sum_{i=1}^n\tilde{\Psi}_i=\bm{0}$.

Now, the estimating equations $\sum_i\tilde{\Psi}_i=\bm{0}$ are algebraically equivalent to the estimating equations for a particular OLS fit, after some transformations, the most important of which follows.
Let
\begin{equation}\label{eq:ri}
\ri=Z_iS_i+(1-Z_i)\ppi=\begin{cases}
\ppi &Z_i=0\\
\sti &Z_i=1
\end{cases}
\end{equation}
In general, $(1-Z_i)\ppi=(1-Z_i)\ri$ and $Z_i\sti=Z_i\ri$, allowing $\ri$ to take the place of $\ppi$ and $S_i$ throughout $\tilde{\Psi}_i$.

\begin{prop}\label{prop:reg1}
  Under Assumptions \ref{ass:sm}, \ref{ass:rand}, \ref{ass:vps}, \ref{ass:ci}, and suitible regularity conditions \citep[e.g.][p. 327]{boosStefanskiBook}, if %principal scores are known exactly,
  $\ri$ is defined as in \eqref{eq:ri} and the model
\begin{equation}\label{eq:regression0}
  \EE[Y_i|Z_i,\ri]=\beta_0+\beta_1\ri+\beta_2Z_i+\beta_3Z_i\ri
\end{equation}
is fit with OLS, yielding coefficient estimates $\hat{\beta}_k$, $k=0,\dots,3$, then
\begin{equation}
  %\begin{split}
    \heff0_{CI}\equiv \hat{\beta}_2\mbox{ and }
    \heff1_{CI}\equiv \hat{\beta}_3+\hat{\beta}_2
%  \end{split}
\end{equation}
are M-estimators, with $\heff0_{CI}\rightarrow_p\eff0$ and $\heff1_{CI}\rightarrow_p\eff1$ as $n\rightarrow\infty$
\end{prop}

Thus, given principal scores, Assumption \ref{ass:ci} enables an analyst to estimate principal effects with a simple regression.
Proposition \ref{prop:reg1} builds on Theorem 2 of \citet{jiangDing2021} which establishes identification of $\eff0$ and $\eff1$ by representing the principal effect estimators as a familiar regression using estimated principal scores.

% If principal scores were known exactly, the M-estimation structure of $\heff0$ and $\heff1$ implies that their sampling variance could be estimated by the standard sandwich estimator for OLS \citep{stefanskiBoos}.
% This will rarely be the case.
%However, 
If the principal scores are modeled as \eqref{eq:prinScoreMod}, and parameters $\bm{\alpha}$ are estimated by M-estimation, with estimating equations $\sum_{i: Z_i=1}\Omega_i=\bm{0}$, then the estimating equations for the principal score model and principal effect estimation can be stacked \citep[c.f.][]{boosStefanskiBook}, as
\begin{equation}\label{eq:stacked}
  \sum_{i=1}^n \begin{pmatrix}
    Z_i\Omega_i(\bxsi,S_i;\bm{\alpha})\\
    \Psi_i\left(Y_i,Z_i,\ri(\bm{\alpha}); \bm{\beta}\right)\end{pmatrix}=\begin{pmatrix} \bm{0}\\\bm{0}\end{pmatrix}
\end{equation}
where $\ri\left(\bm{\alpha}\right)=S_iZ_i+\ppip{\bm{\alpha}}(1-Z_i)$ emphasizes $\ppi$'s dependence on $\bm{\alpha}$.
In practice, the principal score model \eqref{eq:prinScoreMod} could be fit first, yielding estimates $\bm{\hat{\alpha}}$, and model \eqref{eq:regression0} could be fit next, using $\ri=\ri\left(\bm{\hat{\alpha}}\right)$.
Together, the vector of estimates $[\bm{\hat{\alpha}},\bm{\hat{\beta}}]$ represents a zero of the stacked estimating equations \eqref{eq:stacked}.

Then, the standard error matrix for $[\bm{\hat{\alpha}},\bm{\hat{\beta}}]$ can be estimated as \citep[][ch. 7]{boosStefanskiBook}:
\begin{equation}\label{eq:sandwich}
  \widehat{var}(\bm{\hat{\beta}})=A^{-1}BA'
\end{equation}
Where
\begin{equation*}
  A=\sum_i\frac{\partial}{\partial [\alpha,\beta]'} [Z_i\Omega_i,\Psi_i]'
\end{equation*}
and
\begin{equation*}%\label{eq:Bmat}
  B=\sum_i [Z_i\Omega_i, \Psi_i]'[Z_i\Omega_i, \Psi_i]
\end{equation*}
where $[Z_i \Omega_i, \Psi_i]$ is the stacked vector of estimating equations evaluated at $Z_i$, $\bxsi$, $S_i$, and $Y_i$.
The appendix gives details of this calculation, giving a consistent variance estimator.

\subsection{Relaxing Covariate Ignorability with Outcome Modeling}\label{sec:regression}
In most applications, identifying a set of covariates $\bxs$ satisfying CI will be difficult or impossible.
Fortunately, CI may be relaxed or obviated by regression (see \citealt[][\S 3.4]{jiangDing2021} for an analogous identification result).

The assumption we'll state here is stronger than necessary, but leads to an attractively simple method; a weaker version of the assumption and corresponding method, along with the proof, can be found in an online appendix.

\begin{ass}[Residualized Covariate Ignorability]\label{ass:rci}
There exists $\bxy$, a (known) transformation of covariates $\bx$, and an (unknown) vector of coefficients $\bm{\gamma}$ such that
\begin{equation}\label{eq:ass:rci}
\begin{split}
\EE[\yc-\bm{\gamma}'\bxy|\bxs,\st]&=\EE[\yc-\bm{\gamma}'\bxy|\st]\\
\EE[\yt-\bm{\gamma}'\bxy|\bxs,\st]&=\EE[\yt-\bm{\gamma}'\bxy|\st]
\end{split}
\end{equation}
\end{ass}
That is, CI may not hold for potential outcomes themselves, but an analogous assumption holds for \emph{residualized} potential outcomes---i.e. $\yc$ and $\yt$ after subtracting out a linear function of (possibly transformed) covariates.
By forcing $\bm{\gamma}$ to be the same for both treatment groups, and (implicitly) for both principal strata, Assumption \ref{ass:rci} assumes that there are no interactions between columns of $\bxy$ and either condition or principal stratum.
This is akin to the ``Additivity of Treatment Assignment Effect'' assumption in \citet{jo2002}.
Proposition \ref{prop:interactions} in the online appendix relaxes these requirements, but some preliminary simulations suggest that standard errors from estimates allowing for these interactions will be prohibitively large. 
Simulation results in the following section address some cases in which interactions are present in the data generating model but not in the data analysis. 

In any event, Assumption \ref{ass:rci} allows principal effects to be estimated with an OLS model including $\bxy$:

\begin{prop}{GEEPERs}\label{prop:reg2}

Under Assumptions \ref{ass:sm}, \ref{ass:rand}, \ref{ass:vps}, and \ref{ass:rci}, let principal scores be estimated as in \eqref{eq:prinScoreMod} using data from the treatment group and assume they are linearly independent of $\bxy$. Then say the researcher fits the following model with OLS:
\begin{equation}\label{eq:reg2}
Y_i=\beta_0+\beta_1\ri+\beta_2 Z_i+\beta_3Z_i\ri+\bm{\gamma_1}'\bxy_i+\epsilon_i
\end{equation}
where $\ri$ is defined as in \eqref{eq:ri}. 
Then let 
\begin{equation}\label{eq:prinEffEst}
  \begin{split}
    \heff0_{RCI}&\equiv \hat{\beta}_2\\
    \heff1_{RCI}&\equiv \hat{\beta}_2+\hat{\beta}_3
  \end{split}
   \end{equation}
  $\heff0_{RCI}$ and $\heff1_{RCI}$ are M-estimators, with $\heff0_{RCI}\rightarrow_p\eff0$ and $\heff1_{RCI}\rightarrow_p\eff1$ as $n\rightarrow\infty$.
  Under suitible regularity conditions, they are jointly asymptotically normal, with a variance of the form \eqref{eq:sandwich}.
\end{prop}
In short, researchers can estimate principal effects under strong monotonicity by simply imputing missing $\st$ values with estimated principal scores, fitting an OLS regression, and estimating standard errors with a sandwich formula. 
For the remainder of the paper, we will refer to principal effect estimators $\heff1_{RCI}$ and $\heff0_{RCI}$---our preferred estimators---as ``GEEPERs.'' 

\section{A Simulation Study}\label{sec:simulation}

We conducted a simulation study to compare the performance of GEEPERs compared to a Bayesian mixture model and to a principal score weighting estimator, in both favorable and unfavorable circumstances.
The study was designed to answer three sets of overarching questions.
First, how does the performance of the GEEPERs estimator vary under different conditions, including sample size, the extent to which $\bx$ predicts $\st$, whether and how the principal effects themselves vary, and when important interactions are omitted from the outcome model.
Second, how does GEEPERs compare with mixture modeling? Is it competitive in circumstances favorable to both techniques? Does it avoid the pitfalls of mixture modeling when the parametric assumptions of the normal mixture model fail?
Third, are there ways or circumstances when the principal score weighting estimator outperforms GEEPERs even though principal ignorability does not hold?



\subsection{Simulation Design}
The simulation study was conducted in \texttt{R} \citep{rcite} and Stan \citep{rstan}, and full replication code is available at \url{www.github.com/[Redacted]}.
\subsubsection{Data Generation}

In each run of the simulation, we simulated three independent standard normal covariates, $x_k$ $k=1,\dots,3$. However, only the first two covariates were ``observed,'' i.e. were included in the analysis model.
Given the covariates, the (true) principal scores were defined as
\begin{equation*}
  \ppi=logit^{-1}\left[\alpha(x_{1i}-x_{2i}+x_{3i})\right]
\end{equation*}
where $\alpha$ is a manipulated factor.
When $\alpha$ was higher, $\st$ was more easily predicted by covariates; when $\alpha=0$, Assumption \ref{ass:vps} was violated, i.e. $Var(\pp)=0$.
Principal stratum $\sti\in \{0,1\}$ was simulated as $S_i\sim Bern(\ppi)$

Potential and observed outcomes were generated as 
\begin{equation}\label{eq:y-sim}
Y_i=\beta_1\sti+\beta_3Z_i\sti+(\gamma_1+\gamma_2\sti+\gamma_3Z_i)(x_{1i}+x_{2i})+\frac{1}{\sqrt{6}}x_{3i}+\epsilon_i%\\
\end{equation}
%   \begin{split}
%   \yci&=\beta_1\sti+(\gamma_1+\gamma_2\sti)(x_{1i}+x_{2i})+\frac{1}{\sqrt{6}}x_{3i}+\epsilon_i\\
%   \yti&=0.3\sti+(\gamma_1+\gamma_2\sti+\gamma_3Z_i)(x_{1i}+x_{2i})+\frac{1}{\sqrt{6}}x_{3i}+\epsilon_i\\
%   Y_i&=Z_i\yti+(1-Z_i)\yci
%   \end{split}
% \end{equation}
with $\beta_3=0.3-\beta_1$, $\EE[\epsilon_i]=0$, and $Var(\epsilon_i)=1/2$. When $\gamma_3=0$ (i.e. no interaction between $Z$ and covariates), there is no average treatment effect in the $\st=0$ stratum.

The distribution of $\epsilon_i$, $\beta_1$, and coefficients $\gamma_1$, $\gamma_2$, and $\gamma_3$ were manipulated factors in the study.


\subsubsection{Manipulated Factors}
We manipulated six factors in the simulation, listed in Table \ref{tab:factor}.
The six factors are not completely crossed---in particular, we let $n$ and $\alpha$ each vary across all the levels listed while holding the other five factors fixed.

\begin{table}
    \caption{\label{tab:factor} Manipulated factors in simulation study}
  \centering
\begin{tabular}{*{2}{c}}
  \hline
  Factor &Levels\\
  \hline
Sample size per condition $n$ &$100,200,\dots,1000$\\
$\alpha$ &$0,0.2,0.3,\dots,1$\\
The distribution of $\epsilon_i$& Normal, Uniform\\
$\beta_1$ & 0, 0.3\\
Interactions between $\st$ and covariates & No, Yes\\
Interactions between $Z$ and covariates& No, Yes\\
\hline
  \end{tabular}
  \end{table}

\textbf{\emph{Sample Size per condition $\bm{n}$:}} The guarantees of M-estimation are all asymptotic, so we examined the behavior of our GEEPERs at a range of sample sizes to determine its finite-sample properties.

$\bm{\alpha}$\textbf{:} As mentioned above, $\alpha$ controls the variance of the principal scores, i.e. the extent to which $\st$ is predictable as a function of covariates. When $\alpha=0$, the covariates are unrelated to $\st$ and principal scores do not vary between participants, violating \eqref{ass:vps}. We expect that the more predictive the covariates, the better the performance of all three estimators.
To aid in the interpretation of the $\alpha$ parameter, the online appendix plots $\alpha$ against the area under the receiver operating characteristic curve for each fitted model (AUC), a common measure of classification accuracy \citep{bradley1997use}---an AUC of 0.5 implies that the model is no better than random guessing, and AUC of 1 implies perfect prediction. For $0.5\le \alpha \le 1$, the average AUC varies from roughly 0.53 to 0.77; for $\alpha=0.5$, the average AUC was roughly 0.675.


\textbf{\emph{The distribution of $\bm{\epsilon_i}$:}} $\epsilon$ is the regression error in \eqref{eq:y-sim}. Across simulation runs, $E[\epsilon]=0$ and $Var(\epsilon)=1/2$. However, the shape of $\epsilon$'s distribution varied between the following two possibilities:
Normal $\epsilon\sim\mathcal{N}(0,1/2)$, as assumed by the mixture model estimator, or Uniform $\epsilon\sim\mathcal{U}(-\sqrt{6}/2,\sqrt{6}/2)$.
%\item[Mixture] $f_\epsilon(x)=\frac{1}{4}\phi\left((x+1/3)\sqrt{6}\right)+\frac{1}{4}\phi\left((x-1)\sqrt{6})\right)$
%The mixture distribution for $\epsilon$ was designed to be particularly difficult for the mixture model estimator.

$\bm{\beta_1}$\textbf{:} When $\beta_1=0$ there is only one mixture component in the control
group, which can be problematic for mixture modeling \citep{griffin2008application,feller2016principal}. Conversely, when $\beta_1=0.3$, unless there is an interaction between $Z$ and covariates (i.e. $\gamma_3\ne 0$), $\eff0=\eff1=0$, so there is no average treatment effect for either principal stratum.


\textbf{\emph{Interactions between $\st$ or $Z$ and covariates:}} The last two factors---the presence of interactions between covariates and $\st$ and $Z$, respectively---test the performance of a model that does not include interactions when that modeling assumption is false.
When neither interaction was present, $\gamma_2=\gamma_3=0$, then $\gamma_1=1/\sqrt{6}$.
Then, since  $Var(\gamma_1(x_1+x2+x_3))=1/2$ and $Var(\epsilon_i)=1/2$, covariates explained half of the variance of $Y$ within treatment group and principal stratum, and observed covariates $x_1$ and $x_2$ explained 1/3 of that variance.
When there was an interaction between covariates and $\sti$, then $\gamma_1=3/(4\sqrt{6})$ and $\gamma_2=1/(2\sqrt{6})$, so that the slopes for $x_1$ and $x_2$ varied by half their magnitude between principal strata, and on average they were equal to $1/\sqrt{6}$, as in the no-interaction case.
Interactions between covariates and $Z$ were controlled by $\gamma_3$: when there were interactions, $\gamma_3=1/(2sqrt{6})$, otherwise $\gamma_3=0$.
Either type of interaction violates Assumption \ref{ass:rci} since if $\gamma_2\ne 0$ or $\gamma_3 \ne 0$, there is no single vector $\bm{\gamma}$ that can fully capture the dependence of $\yc$ and $\yt$ on $S$---instead, the dependence varies with $\st$ and/or with $Z$. 

\subsubsection{Analysis Models}\label{sec:simMods}

Using each simulated dataset, we estimated $\eff0$ and $\eff1$ with three methods: a mixture model \eqref{eq:mixtureModelPS} including principal scores, GEEPERs, and third, the principal score weighting estimator \eqref{eq:psw}.

The methods all used the same ``observed'' dataset, consisting of outcomes $Y$, treatment assignments $Z$, two covariates $\bxs=\bxy=\bx=[x_1,x_2]$, and $S$ observed only when $Z=1$.
The third covariate used in the data generating model, $x_3$, was ``unobserved''--since $x_3$ was correlated with both $\st$ and $Y$, Principal Ignorability (Assumption \ref{ass:PI}) was violated.

All three methods used the same principal score model, a logistic regression of $S$ on $\bx$, and an intercept, fit using data from the treatment group in which $S=\st$ was observed, i.e. $\ppf{\bm{\alpha}}{\bx}=logit^{-1}(\alpha_0+\bm{\alpha}'\bx)$.

The mixture estimator then fit two outcome models:
For the treatment group, the regression model $Y=\beta_0^T+\beta_1^TS+\bm{\gamma}'\bx+\epsilon$ and for the control group, a mixture model of the form \eqref{eq:mixtureModelPS}, but with $\muc0$ and $\muc1$ replaced by regression functions $\beta_0^C+\bm{\gamma}'\bx$ and $\beta_0^C+\beta_1^C+\bm{\gamma}'\bx$, respectively.
Residual variance were allowed to differ across treatment groups, but not across principal strata.
These models were fit simultaneously using Bayesian Markov Chain Monte Carlo with Stan \citep{rstan}, and principal effects $\eff0$ and $\eff1$ estimated as $\beta_0^T-\beta_0^C$ and $\beta_0^T+\beta_1^T-\beta_0^C-\beta_1^T$, respectively. We interpreted the posterior mean as a point estimate and the posterior standard deviation as a standard error.


In contrast, for GEEPERs and principal score weighting the principal score model was fit by itself, to data from the treatment group, using standard maximum likelihood methods.
For GEEPERs, we used the estimated principal scores in the control group, along with the observed $S$ in the treatment group, to construct $R$, and then fit the model \eqref{eq:reg2} with OLS. 
To estimate standard errors, we used the \texttt{sandwich} package \citep{sandwich} to extract estimating equations from \eqref{eq:psModsim} and \eqref{eq:simReg} and computed the sandwich covariance matrix following the procedure detailed in the appendix.

For principal score weighting we plugged estimated principal scores in to \eqref{eq:psw} to estimate $\muc0$ and $\muc1$, estimated $\mut0$ and $\mut1$ with the sample means of subjects with $Z=1$ and $S=0$ or $S=1$, respectively, and estimated $\heff0=\hat{\mut0}-\hat{\muc0}$ and $\heff1=\hat{\mut1}-\hat{\muc1}$.

\subsection{Results}
In this section we present the subset of simulation results that we think is the most telling and interesting; a fuller set of results is available in the online appendix.

\subsubsection{Varying $n$ and $\alpha$}
When $\alpha=0$, Assumption \ref{ass:vps} is violated, and M-estimates are highly erratic; however, as Table \ref{tab:coverage} shows, when other assumptions hold inference remains valid.

Figure \ref{fig:alphan} shows the empirical bias and standard error (SE) of the GEEPERs and mixture estimator of $\eff1$ as sample size and $\alpha$ vary.
Across $n$ and $\alpha$, GEEPERs has lower bias and higher SE than the mixture estimator.
GEEPERs is positively biased for low $n$ with bias converging to close to 0 (up to simulation error) for $n\ge 300$, and appears to be roughly unbiased (up to simulation error) for $\alpha \ge 0.2$, with $n$ fixed at 500.
The SE of the GEEPERs decreases rapidly for $100\le n \le 500$ and $0.2\le \alpha \le 0.5$ and more gradually for higher values of $n$ and $\alpha$.

For the remainder of the simulation, we focus on the moderate cases of $n=500$ and $\alpha=0.2$ or 0.5.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{../simFigs/biasSEbyB1N.jpg}
  \caption{Empirical bias and standard error of GEEPERs and Mixture estimates of $\muc1$ as sample size per treatment group $n$ or $\alpha$ varies. There were 500 replications for each $n$ or $\alpha$. Other factors were held fixed at the noted values.}
  \label{fig:alphan}
\end{figure}

% \begin{figure}
%   \centering
%   \includegraphics{../simFigs/biasSEbyB1.jpg}
%   \caption{Empirical bias and standard error of M- and Mixture estimates of $\muc1$ as $\alpha$ varies from 0.1 to 1. There were 500 replications for each $n$. Other factors were held fixed at the noted values.}
%   \label{fig:alpha}
% \end{figure}

\subsubsection{Other Factors}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth,clip]{../simFigs/boxplots.pdf}
  \caption{Violin and scatterplots of 500 simulation estimation errors for estimators described in Section~\ref{sec:simMods} under varying conditions. For all plots, $n=500$; there were 500 replications for each set of conditions. Annotations indicate the number of extreme outliers excluded from the plotting area.}
  \label{fig:boxplots}
\end{figure}

Figure \ref{fig:boxplots} shows violin plots, layered on top of jittered scatterplots, showing estimation error for GEEPERs, the mixture estimator, and the PSW estimator of $\eff1$ under various sets of conditions, and Table \ref{tab:coverage} shows empirical covarage of nominal 95\% confidence interval estimates under a somewhat wider set of circumstances when $\muc1=0.3$.
The leftmost panel of Figure \ref{fig:boxplots} and the top line of Table \ref{tab:coverage} show results conditions favorable to both the mixture model and to GEEPERs---normal residuals and no interactions in the data generating model between covariates and either $S$ or $Z$.
In this case, GEEPERs is roughly unbiased, but less precise than the other two methods---when $\alpha=0.2$ it is much noisier, and when $\alpha=0.5$ it is only slightly less precise.
The mixture estimator's bias depends on $\muc1$---when $\muc1=0$, the bias is slight, and when $\muc1=0.3$ the bias is more substantial. This is somewhat surprising, because when $\muc1=0$, there is no separation between the principal strata in the control group, and previous literature \citep{griffin2008application} has identified this scenario as a particular challenge for mixture modeling.
The PSW estimator is also biased---due, presumably, to the violation of Principal Ignorability, Assumption \ref{ass:PI}---inherent in the data generating model. On the other hand, the PSW estimator has easily the lowest sampling variance of the three estimators.
The middle panel of Figure \ref{fig:boxplots} and the fifth line of Table \ref{tab:coverage} show a similar set of results for when the residuals in \eqref{eq:y-sim} are uniformly distributed.
In this case, GEEPERs and PSW estimator behave in a similar fashion to the normal case, but the mixture estimator shows a bimodal pattern--sometimes over-estimating $\eff1$ and sometimes underestimating, but rarely estimating $\eff1$ accurately.
Credible intervals from the mixture model exhibit severe undercoverage, while GEEPERs confidence intervals achieve at least their nominal level.

%\sloppy
The rightmost panel of Figure \ref{fig:boxplots} returns to the case of normal residuals---and only shows results when $\muc1=0.3$ and $\alpha=0.5$---but introduces interactions in the data generating model between covariates and either $\st$, $Z$, or both. That is, the coefficients on the covariates in the outcome model vary between principal strata and/or between treatment groups, violating Assumption \ref{ass:rci}..
%Presence of interactions could undermine the GEEPERs by inducing a relationship between principal scores and potential outcomes, even after adjusting the outcomes for covariate effects.
Fortunately, the presence of interactions in the data generating model in this scenario appear to make little difference when $\alpha=0.5$; however, Table \ref{tab:coverage} shows that when $\alpha=0$ or 0.2, and there are interactions between $\bx$ and $Z$ (i.e. $\gamma_3\ne 0$) GEEPERs 95\% confidence intervals tend to under-cover. Nevertheless, coverage of GEEPERs intervals is nearly always substantially higher than coverage of corresponding 95\% credible intervals from mixture models.%\footnote{}



\begin{table}

\caption{\label{tab:coverage}Empirical coverage of nominal 95\% confidence intervals for GEEPERs and Bayesian mixture modeling under varying conditions.}
\centering
\begin{tabular}{*{9}{c}}%*{6}{r}}
\toprule
\multicolumn{3}{c}{ } & \multicolumn{2}{c}{$\alpha=0$} & \multicolumn{2}{c}{$\alpha=0.2$} & \multicolumn{2}{c}{$\alpha=0.5$} \\
\cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7} \cmidrule(l{3pt}r{3pt}){8-9}
\makecell[l]{Residual\\Dist.} & \makecell[c]{X:Z\\Int.?} & \makecell[r]{X:S\\Int.?} & \rotatebox[origin=c]{300}{GEEPERs} & \rotatebox[origin=c]{300}{Mixture}  & \rotatebox[origin=c]{300}{GEEPERs} & \rotatebox[origin=c]{300}{Mixture}& \rotatebox[origin=c]{300}{GEEPERs} & \rotatebox[origin=c]{300}{Mixture}\\
\midrule
 & No & No & 0.99 & 0.99  & 0.97 & 0.97 & 0.96 & 0.96\\

 & Yes & No & 0.88 & 0.74  & 0.88 & 0.74 & 0.94 & 0.89\\

 & No & Yes & 1.00 & 1.00  & 0.98 & 0.97 & 0.97 & 0.96\\
\multirow{-4}{*}{\raggedright\arraybackslash Normal} & Yes & Yes & 0.90 & 0.77  & 0.90 & 0.77 & 0.94 & 0.88\\
\cmidrule{1-9}
 & No & No & 0.99 & 0.29  & 0.97 & 0.40 & 0.95 & 0.61\\

 & Yes & No & 0.83 & 0.11  & 0.87 & 0.20 & 0.93 & 0.48\\

 & No & Yes & 1.00 & 0.47  & 0.98 & 0.53 & 0.96 & 0.57\\

\multirow{-4}{*}{\raggedright\arraybackslash Uniform} & Yes & Yes & 0.83 & 0.15 & 0.88 & 0.21 & 0.94 & 0.53\\
\bottomrule
\multicolumn{9}{p{0.9\textwidth}}{\footnotesize Standard error $\approx 1$ percentage point, based on 500 replications and true coverage probabilities of 0.95.}

\end{tabular}
\end{table}


\section{Application: Estimating Treatment Effects for Implementation Subgroups}\label{sec:fh2t}

In a recent educational field experiment, middle school students completing their math schoolwork on a computer program were randomized between four conditions. In all four conditions, students worked on their assigned math program in class on 12 different occasions---a prior assessment, a mid-test, a post-test, and nine learning sessions---spaced throughout the school year.  
The four conditions included two gamified programs---DragonBox and From Here to There (FH2T)---and two programs in which students used computers to work through a series of algebra problems taken from the district's textbook---``Assistments'' and an active control called ``Business as Usual'' (BAU).  % the active control condition in which students simply used the computer to work on a sequence of math problems. 
%In the DragonBox game, students learned to solve algebraic equations by isolating a box containing a dragon; as students progressed in the game, pictures of monsters were replaced by mathematical symbols. 
%Another condition, the From Here to There (FH2T) program, used visual cues such as color and spacing to teach algebraic concepts, and allowed students to solve equations by manipulating expressions via dragging and dropping.
%In the ASSISTments condition, students used computers to work through a series of math problems taken from the district's textbook. 
Students in the ASSISTments condition had access to a series hints for each problem, culminating in a ``bottom out'' hint that contained the answer and were given immediate feedback on their answers. % and had to enter the correct answer in order to proceed to the next problem. 
In the BAU condition students had no access to hints and received feedback only after completing the assignment. 
In this illustration, we will compare the ASSISTments condition (which we arbitrarily label ``treatment'')---the only condition featuring bottom-out hints---with each of the three others separately. Our goal is to estimate different treatment effects for groups of students who would, if assigned to the ASSISTments condition, request few or many bottom out hints. 

The literature on hints in online learning is mixed \citep[see, e.g.,][]{aleven2016help,goldin2012learner,sales2021student} in particular when it comes to requesting a bottom-out hint (called ``bottoming out''). On the one hand, hints that include the correct answer are essentially worked examples, which can be beneficial to learning \citep[e.g.]{sweller1985use}. On the other hand, they allow students to ``game the system,'' by simply requesting enough hints to see the right answer, and use the software without ever working to solve any problems \citep[e.g.][]{guo2008trying}. However, in this experiment, the ability to bottom out was only one of several differences between the treatment conditions. One way to disentangle the role of bottoming out in any treatment effect would be to estimate average treatment effects of being assigned to the ASSISTments condition, relative to any of the other three, separately for subjects who would (if assigned to treatment) bottom out frequently, and for subjects who would not. 
If the availability of bottom-out hints plays an important positive role in ASSISTments's efficacy, we might expect larger effects of condition on students who would bottom out frequently than for those who would not; if bottom-out hints are harmful, we might expect the opposite. 
%If hints (that do not supply the correct answer) and error messages are helpful, but the ability to bottom out is harmful to learning, then we might expect to estimate a positive effect of being assigned to the feedback condition for students who do not bottom out, but a negative effect for students who do. If hints are helpful for learning only if they can be read as worked examples, then we might expect no effect of condition on students who generally do not bottom out, but a positive effect for students who do. 
(These hypothetical results may be masked by other treatment effect heterogeneity between students who would or wouldn't bottom out, but the results could still be informative.)

\sloppy
For the purpose of demonstrating GEEPERs, we gathered dichotomized data on %which students in the treatment condition requested at least one bottom-out hint.
whether students in the treatment condition requested more than the median number of bottom-out hints (11).
That is, $\st=1$ for students who, if assigned to the treatment condition, would request at more than 11 bottom-out hints (``bottom-outers''), and $\st=0$ for students who would request 11 or fewer (``non-bottom-outers''). %least one bottom-out hint, and $\st=0$ for students who would not.
The outcome of interest $Y$ in our example is students' total scores on a ten-item posttest completed within the online tutoring system; $Y$ is an integer ranging from 0 to 10.
The goal of our analysis will be to determine the principal effects $\tau_1=\EE[\yt-\yc|\st=0]$ and $\tau_0=\EE[\yt-\yc|\st=0]$, the average effects of assignment to the treatment condition for bottom-outers and non-bottom-outers.

We also had access to data on a number of baseline covariates, including prior achievement, demographics, baseline measurements of student attitudes towards math and an indicator for whether students began the school year in remote or in-person instruction.
Students with missing pretest scores were dropped from the analysis; missing data in other covariates was imputed with a Random Forest algorithm \citep{missForest}.

The data analysis we present here is intended as a demonstration of GEEPERs, rather than for its substantive conclusions, and our description omits discussion of some important methodological considerations, including attrition and post-selection inference. For a more complete discussion of the experimental design, the conditions being compared, and the impact analysis, please see \citet{impactPaper}.

Due to privacy regulations, the data are not publicly available; however, interested analysts may access anonymized data by emailing the study's principal investigator at [REDACTED] and signing a privacy agreement.
Replication code for the analysis is available at \url{www.github.com/[Redacted]}.

\subsection{Data Analysis}
\subsubsection{Principal Score Model}\label{sec:psMod}

% \begin{figure}
%   \centering
%   \includegraphics{../figure/psModCoef.jpg}
%   \caption{Principal Score Model Coefficients}
%   \label{fig:psMod}
% \end{figure}


We estimated principal scores using logistic regression of observed $S$ on schol fixed effects and a set of baseline variables chosen with a combination of expert judgement and backwards selection based on the AIC, and evaluated model fit with binned residual plots \citep{gelman2006data}.
The model was fit to data from students in the treatment group, for whom bottom-out status ($S$) was observed; hence, the same model fit was used for all three treatment contrasts.
The model was fairly successful in distinguishing bottom-outers from non-bottom-outers---the AUC, evaluated using out-of-sample predictions in a 10-fold cross-validation, was roughly 0.795. That is, in roughly 80\% of bottom-outer/non-bottom-outer pairs of subjects, the bottom-outer will have a higher predicted probability from the model.


% \subsubsection{Outcome Regression and Principal Effects}\label{sec:outMod}
% \begin{figure}
%   \centering
%   \includegraphics{../figure/prinEffs.pdf}
%   \caption{Principal Effects}
%   \label{fig:effects}
% \end{figure}

% Figure \ref{fig:effects} plots the estimated principal effects for bottom-outers and non-bottom-outers, using two different models---alongside estimates of the overall average treatment effect, labeled ``ATE.'' In both models, effects were adjusted for all of the variables that were in the principal score model, in addition to students' pretest scores; also, the models substituted class fixed effects for school fixed effects. %\footnote{Why use a coarser set of fixed effects for the principal score model than the outcome regression? There were two reasons: first, the principal score model was fit to a smaller sample (only students in the ``Instant'' condition), so some classrooms only included one student, which can lead to poor fit. Second, the asymptotics of logistic regression are more restricted than OLS, and so logistic regression tends to perform poorly when the number of parameters increases with the sample size \citep{agresti}.}
% The two sets of estimated differed in that the first assumed that coefficients on covariates were equal across principal strata, whereas the second set of results allowed the coefficients on pretest and 5th-grade standardized test scores---the most important predictors of outcomes and principal strata membership, respectively---to vary between strata.

% The results suggest little difference, if any, between the effects in the two principal strata.
% Estimated effects compared to BAU or DragonBox were slightly lower for bottom-outers than for non-bottom-outers, and estimated effects compared to FH2T were higher for non-bottom-outers.
% However, none of these differences is statistically significant, so opposite patterns, or no difference between principal effects at all, would also be consistent with the data.
% In fact, 95\% confidence intervals for all principal effects include both positive and negative principal effects.
% There is no discernible difference between estimated principal effects when coefficients on pretest and grade-5 standardized tests were or were not allowed to vary between strata, though standard errors from the models including an interaction were somewhat higher.

\begin{figure}
  \centering
  \includegraphics{../figure/compareMethods.pdf}
  \caption{Principal Effects}
  \label{fig:compare}
\end{figure}

Figure \ref{fig:compare} shows principal effect estimates from GEEPERs compared to analogous estimates using Bayesian mixture modeling and PSW, alongside estimates of the overall average treatment effects, labeled ``ATE.''.
All three sets of estimates relied on the same principal score model described above, in Section \ref{sec:psMod}, though in the mixture model it was fit simultaneously with effect estimation using a Markov Chain Monte Carlo algorithm \citep{rstan}.
Outcome regressions in GEEPERs and the mixture model included all of the covariates that were in the principal score model, in addition to students' pretest scores; also, the models substituted class fixed effects for school fixed effects. 

The Bayesian mixture model assumed that outcomes were normally distributed, conditional on covariates (the same as in Section \ref{sec:outMod}) and principal stratum, with a residual standard deviation that varied between principal strata. This modeling assumption is necessarily false, since post-test scores were integers between 0 and 10. All model parameters with support in $\mathbb{R}$ were given standard normal priors, while standard deviation parameters were given half-standard normal priors.

The PSW estimators did not include any covariate outcome modeling. Standard errors for the PSW estimates were estimated using the bootstrap, with resampling done within schools.

For all three sets of estimators, approximate 95\% confidence intervals were estimated by adding and subtracting twice the standard error from the point estimate (or twice the posterior standard deviation from the posterior mean, in the Bayesian mixture model).

The GEEPERs results suggest little difference, if any, between the effects in the two principal strata. Estimated effects compared to BAU or DragonBox were slightly lower for bottom-outers than for non-bottom-outers, and estimated effects compared to FH2T were higher for non-bottom-outers.
However, none of these differences is statistically significant, so opposite patterns, or no difference between principal effects at all, would also be consistent with the data.
In fact, 95\% confidence intervals for all principal effects include both positive and negative principal effects.

The other estimation strategies largely agreed with GEEPERs, and with each other. 
This may suggest that the estimators' various assumptions---additivity of treatment assignment effect for GEEPERs and the mixture model, normal residuals for the mixture model, and principal ignorability for PSW---were all innocuous and may have held approximately. Alternatively, violations of the different assumptions may have led to similar errors.


\section{Discussion}\label{sec:discussion}
GEEPERs is a straightforward approach to principal effect estimation under strong monotonicity, built on widely-used regression models, which is more robust---though less precise---than alternative approaches under a wide array of scenarios.

There is good reason to hope that extensions to GEEPERs may be straightforward.
First, GEEPERs may be extended to situations in which there is two-way noncompliance, and only a weaker form of monotonicity (Assumption \ref{ass:sm}) holds.
For instance, in truncation by death problems \citep[e.g.][]{zhangRubin,ding2011}, $S_i=1$ if participant $i$ survives (or, more generally, if the outcome $Y_i$ is measured) and 0 otherwise, and interest is is in the principal effect for the ``always survive'' principal stratum in which $\sti=S_{ci}=1$.
If, say, $\sti\ge S_{Ci}$, then every subject in the control condition with $S_i=1$ is in the always survive stratum, while those subjects in the treatment condition with $S_i=1$ are a mixture of the always survive and $\sti=1;\;S_{Ci}=0$ stratum.
The scenario of truncation by death with (weak) monotonicity is broadly similar to the strong monotonicity, one-way noncompliance situation that this paper discussed.
On the other hand, when no monotonicity assumption holds, both $\st$ and $S_C$ will have to be imputed for every subject; further research is necessary to determine the appropriate way to do so.

Another direction of extension involved the principal score model \eqref{eq:pscore}.
The performance of GEEPERs in the simulation study of Section \ref{sec:simulation} depended heavily on the factor $\alpha$, which controlled the extent to which covariates could predict $S$.
That suggests that when covariates are high-dimensional, GEEPERs' performance in applications could be optimized with a high-dimensional semi- or non-parametric model.
If so, several further questions emerge.
First, can the parameter $\alpha$ be extended to a more general parameter measuring the prediction accuracy of a non-parametric model.
Second, if the principal score model cannot be formulated as the solution to a set of estimating equations, how should the standard error be computed?
Lastly, can over-fit principal score models cause bias or other estimation problems, and if so, are there ways to protect against overfitting?

GEEPERs is a flexible, easily-implementable method for principal effect estimation for one-way noncompliance, with predictive covariates; extensions to a broader set of circumstances could be a boon to causal modeling.

\bibliographystyle{plainnat}
\bibliography{MOM}
\appendix

\section{Proofs and Calculations}
\subsection{Proof for Lemma \ref{lemma:expectation}}
As a preliminary, note that
% \begin{align*}
%   \EE[\st|\hpp]&=\EE\left\{\EE[\st|\pp,\hpp]|\hpp\right\}\\
%              &=\EE\left\{\EE[\st|\pp]|\hpp\right\}\\
%              &=\EE[\pp|\hpp]=\hpp
% \end{align*}

%Then, note
\begin{align*}
  \EE[Y_C|\pp]&=\EE\left\{\EE[Y_C|\pp,\st]|\pp\right\}\\
             &=\EE\left\{\EE[Y_C|\st]|\pp\right\}\tag*{by \eqref{eq:assumption}}\\
             &=\EE[\muc1\st+\muc0(1-\st)|\pp]\\
             &=\muc1\pp+\muc0(1-\pp)
\end{align*}

Then we have
\begin{equation*}
  \begin{split}
    \EE[Y_C]=&\EE\EE[Y_C|\pp]=\muc1\EE\pp+\muc0(1-\EE\pp)\\
    =&\muc0+\EE\pp(\muc1-\muc0)
    \end{split}
\end{equation*}

Next we have

\begin{align*}
  \EE[Y_C\pp]&=\EE\left\{\EE[Y_C\pp|\pp]\right\}\\
            &=\EE\left\{\pp\EE[Y_C|\pp]\right\}\\
            &=\EE\left\{\pp\left[\muc1\pp+\muc0(1-\pp)\right]\right\}\\
            &=\EE[\pp]\muc0+\EE[\pp^2](\muc1-\muc0)
\end{align*}

In the treatment group, $\st$ is observed, so
\begin{align*}
    \EE[Y_T]=&\mut0+\EE[\st](\mut1-\mut0)\tag*{and}\\
    \EE[\st Y_T]=&\EE[\st]\mut0+\EE[\st^2](\mut1-\mut0)
\end{align*}

Due to Assumption \ref{ass:rand} (randomization), $\EE[Y|Z=0]=\EE[Y_C]$, $\EE[Y|Z=1]=\EE[Y_T]$, $\EE[Y\pp|Z=0]=\EE[Y_C\pp]$ and $\EE[YS|Z=1]=\EE[Y_T\st]$, completing the proof.

\subsection{Proof for Proposition \ref{prop:reg1}}

Replacing $\sti$ and $\pp$ in \eqref{eq:estEq0} with $\ri$, as in \eqref{eq:ri}, and replacing $\tilde{\Psi}_i$ with $\Psi_i=\begin{psmallmatrix} 1 & 0&1&0\\ 0&1&0&1\\0&0&1&0\\0&0&0&1\end{psmallmatrix}\tilde{\Psi}_i$ gives an equivalent set of estimating equations $\sum_{i=1}^{n_C}\Psi_i=\bm{0}$ or 
\begin{equation}\label{eq:estEq1}
\sum_{i=1}^{n_C}\begin{pmatrix}
    Y_i-\muc0-\ri(\muc1-\muc0)-Z_i(\mut0-\muc0)-Z_i\ri(\mut1-\mut0-\muc1+\muc0)\\
    \ri Y_i-\ri\muc0-\ri^2(\muc1-\muc0)-Z_i\ri(\mut0-\muc0)-Z_i\ri^2(\mut0-\mut1-\muc0+\muc1)\\
    Z_iY_i-Z_i\mut0-Z_i\ri (\mut1-\mut0)\\
    Z_i\ri Y_i -Z_i\ri\mut0-Z_i\ri^2(\mut1-\mut0)

\end{pmatrix}=\begin{pmatrix} 0\\0\\0\\0\end{pmatrix}
\end{equation}
These are equivalent to the estimating equations for OLS model \eqref{eq:regression0} with $\beta_0=\muc0$, $\beta_1=\muc1-\muc0$, $\beta_2=\mut0-\muc0$, and $\beta_3=\mut1-\mut0-\muc1-\muc0$.
Therefore, under standard OLS regularity conditions the estimated parameter vector $\bm{\hat{\beta}}$ is consistent, completing the proof.


\subsection{A Stronger Version of Proposition \ref{prop:reg2} and a Proof}



\begin{prop}\label{prop:interactions}
  Say, for $i=1,\dots,n$, principal scores $\ppi$ are generated as \eqref{eq:pscore}, with parameters $\bm{\alpha}$ identified and estimable with M-estimation, and there exist $\beta_0$, $\beta_1$, $\beta_2$, $\beta_3$, $\bm{\gamma_1}$, $\bm{\gamma_2}$, $\bm{\gamma_3}$ amd $\bm{\gamma_4}$ such that
  \begin{equation}\label{eq:interaction}
    \begin{split}
    \EE[Y_i|\st,Z,\bx]=&\beta_0+\beta_1\sti+\beta_2 Z_i+\beta_3Z_i\sti\\
    &+\bm{\gamma_1}'\bxy_i+\bm{\gamma_2}'\bxy_i\sti+
    \bm{\gamma_3}'\bxy Z_i+\bm{\gamma_4}'\bxy_i Z_i\sti
    \end{split}
  \end{equation}
  
  Then, under Assumptions \ref{ass:sm}, \ref{ass:rand}, and \ref{ass:vps}, if $\ppi$ is linearly independent of $\bxy$, a researcher may follow the following procedure to estimate principal effects:
  \begin{enumerate}
  \item Estimate principal scores by fitting model \eqref{eq:pscore} to data from the treatment group
  \item Replace $\sti$ with $\ri$ (as defined in \ref{eq:ri}) in model \eqref{eq:interaction} and fit with OLS
  \item Estimate principal effects as:
   \begin{equation}\label{eq:prinEffEst}
  \begin{split}
    \heff0_{int}&\equiv \hat{\beta}_2+\bm{\hat{\gamma}_3}'\overline{\bxy}_{Z=1,S=0}\\
    \heff1_{int}&\equiv \hat{\beta}_2+\hat{\beta}_3+(\bm{\hat{\gamma}_3}+\bm{\hat{\gamma}_4})'\overline{\bxy}_{Z=1,S=1}
  \end{split}
   \end{equation}
   where $\overline{\bxy}_{Z=1,S=0}$ and $\overline{\bxy}_{Z=1,S=0}$ are the vector of covariate sample means for the subsets of subjects with $Z=1$ and $S=0$ or $S=1$, respectively.
  \end{enumerate}
  Then $\heff0_{G}$ and $\heff1_{int}$ are M-estimators, with $\heff0_{int}\rightarrow_p\eff0$ and $\heff1_{G}\rightarrow_p\eff1$ as $n\rightarrow\infty$.
  Under suitible regularity conditions, they are jointly asymptotically normal, with a variance of the form \eqref{eq:sandwich}.
\end{prop}

Equation \eqref{eq:interaction} implies Assumption \ref{ass:rci} with $\bm{\gamma_2}=\bm{\gamma_3}=\bm{\gamma_4}=0$. 

\begin{proof}
First of all, by \eqref{eq:interaction}, 
\begin{equation*}
  \EE[Y_T-Y_C|\st=0]=\EE[Y|Z=1,\st=0]-\EE[Y|Z=1,\st=0]=\beta_2+\bm{\gamma_3}'\EE[\bxy|\st=0]
\end{equation*}
and
\begin{equation*}
  \EE[Y_T-Y_C|\st=1]=\EE[Y|Z=1,\st=1]-\EE[Y|Z=1,\st=1]=\beta_2+\beta_3+(\bm{\gamma_3}'+\bm{\gamma_4}')\EE[\bxy|\st=1]
\end{equation*}
Furthermore, $\overline{\bxy}_{Z=1,S=0}\rightarrow \EE[\bxy|\st=0]$ and $\overline{\bxy}_{Z=1,S=1}\rightarrow \EE[\bxy|\st=1]$ as $n\rightarrow \infty$.

We will show that the estimated coefficients from model \eqref{eq:interaction}, but with $R$ replacing $\st$, fit with OLS, are consistent for $\bm{\beta}$ and $\bm{\gamma}$ from \eqref{eq:interaction}. 

First, note that $\EE[S]=\EE\EE[S|\bx]=\EE[\pp]$ and
\begin{align*}
  \EE[\bx S]&=\EE[\bx S|Z=1]=\EE[\bx S|Z=0] \mbox{ (due to randomization)}\\
  &=\EE[\bx\EE[S|\bx]|Z=0]=\EE[\bx\pp|Z=0]=\EE[\bx\pp]
\end{align*}
implying that, according to \eqref{eq:interaction},
\begin{equation*}
  \begin{split}
    \EE[Y|\bx,Z=0]&=\beta_0+\beta_1\pp+\bm{\gamma_1}'\bm{X^Y}+\bm{\gamma_2}'\bm{X^Y}\pp\\
                  &=\beta_0+\beta_1R+\bm{\gamma_1}'\bm{X^Y}+\bm{\gamma_2}'\bm{X^Y}R\\
    \EE[Y|\bx,S,Z=1]&=\beta_0+\beta_2+(\beta_1+\beta_3)S+(\bm{\gamma_1}+\bm{\gamma_3})\bxy+(\bm{\gamma_2}+\bm{\gamma_4})\bxy S\\
    &=\beta_0+\beta_2+(\beta_1+\beta_3)R+(\bm{\gamma_1}+\bm{\gamma_3})\bxy+(\bm{\gamma_2}+\bm{\gamma_4})\bxy R
  \end{split}
\end{equation*}
Therefore,
\begin{align*}
  \EE[Y]&=\EE[Y|Z=0]+\EE[Z]\left\{\EE[Y|Z=1]-\EE[Y|Z=0]\right\}\\
  &=\beta_0+\beta_1\EE[R]+\beta_2\EE[Z]+\beta_3\EE[ZR]+\bm{\gamma_1}'\EE[\bxy]+\bm{\gamma_2}'\EE[\bxy R]+\bm{\gamma_3}'\EE[\bxy Z]+\bm{\gamma_4}\EE[\bxy RZ]
\end{align*}
  
Analogous reasoning leads to expressions for $\EE[RY]$, $\EE[\bxy Y]$, $\EE[ZY]$,  $\EE[ZRY]$, $\EE[\bxy YZ]$, and $\EE[\bxy RZY]$.
These, in turn, give rise to estimating equations
\begin{equation*}
\begin{split}
  &\psi_i=\\ &\begin{pmatrix}
  Y_i\\
  \phantom{}\\
  Y_i\ri\\
    \phantom{}\\
  Y_iZ_i\\
  \phantom{}\\
  Y_iZ_i\ri\\
  \phantom{}\\
  Y_i\bxy_i\\
  \phantom{}\\
  Y_i\bxy_i\ri\\
  \phantom{}\\
  Y_i\bxy_iZ_i\\
  \phantom{}\\
  Y_i\bxy_iZ_i\ri\\
  \phantom{}\end{pmatrix} - \begin{pmatrix*}[l]
    \beta_0+\beta_1R_i+\beta_2Z_i+\beta_3Z_iR_i\\
    \quad+\left\{\bm{\gamma_1}'+R_i\bm{\gamma_2}'+Z_i\bm{\gamma_3}'+R_iZ_i\bm{\gamma_4}'\right\}\bxy\\
    \beta_0\ri+\beta_1R_i^2+\beta_2Z_i\ri+\beta_3Z_iR_i^2\\
    \quad+\ri\left\{\bm{\gamma_1}'+R_i\bm{\gamma_2}'+Z_i\bm{\gamma_3}'+R_iZ_i\bm{\gamma_4}'\right\}\bxy\\
    (\beta_0+\beta_2)Z_i+(\beta_1+\beta_3)R_iZ_i\\
    \quad+Z_i\left\{\bm{\gamma_1}'+R_i\bm{\gamma_2}'+Z_i\bm{\gamma_3}'+R_iZ_i\bm{\gamma_4}'\right\}\bxy\\
    \beta_0Z_i\ri+(\beta_1+\beta_3)Z_iR_i^2+\beta_2Z_i\ri\\
    \quad+Z_iR_i\left\{\bm{\gamma_1}'+\bm{\gamma_3}'+R_i(\bm{\gamma_2}'+\bm{\gamma_4}')\right\}\bxy \\
    \beta_0\bxyt+\beta_1R_i\bxyt+\beta_2Z_i\bxyt+\beta_3Z_iR_i\bxyt\\
    \quad {}+\left\{\bm{\gamma_1}'+R_i\bm{\gamma_2}'+Z_i\bm{\gamma_3}'+R_iZ_i\bm{\gamma_4}'\right\}\bxy\bxyt\\
    \beta_0\ri\bxyt+\beta_1R_i^2\bxyt+\beta_2Z_i\ri\bxyt+\beta_3Z_iR_i^2\bxyt\\
    \quad {}+\ri\left\{\bm{\gamma_1}'+R_i\bm{\gamma_2}'+Z_i\bm{\gamma_3}'+R_iZ_i\bm{\gamma_4}'\right\}\bxy\bxyt\\
        (\beta_0+\beta_2)Z_i\bxyt+(\beta_1+\beta_3)R_iZ_i\bxyt\\
    \quad {}+Z_i\left\{\bm{\gamma_1}'+R_i\bm{\gamma_2}'+Z_i\bm{\gamma_3}'+R_iZ_i\bm{\gamma_4}'\right\}\bxy\bxyt\\
\beta_0\ri Z_i\bxyt+\beta_1R_i^2Z_i\bxyt+\beta_2\ri Z_i\bxyt+\beta_3Z_iR_i^2\bxyt\\
    \quad {}+\ri Z_i\left\{\bm{\gamma_1}'+R_i\bm{\gamma_2}'+Z_i\bm{\gamma_3}'+R_iZ_i\bm{\gamma_4}'\right\}\bxy\bxyt\\
  \end{pmatrix*}
  \end{split}
\end{equation*}
with $\EE[\psi_i]=0$.
These are the estimating equations for the regression model \eqref{eq:interaction}, with $R$ replacing $\st$, fit by OLS. 
Under standard OLS regularity conditions, the estimated coefficients of that model are consistent, completing the proof.  
\end{proof}

\subsection{Sandwich Matrix Calculations}

Here we will derive the sandwich variance-covariance matrix for the GEEPERs estimate without interactions between $\bx$ and either $Z$ or $\st$---i.e. with $\bm{\gamma_2}=\bm{\gamma_3}=\bm{\gamma_4}=0$ in the notation of \eqref{eq:interaction}---and estimating principal scores using a generalized linear model. 

We propose estimating principal effects in two stages.
First, fit the model
\begin{equation}\label{eq:psMod}
  \ppi=Pr(\sti=1|\bxsi)=f(\bm{\alpha}'\bxsit)
\end{equation}
for some inverse link function $f(\cdot)$, where $\bxsit=[1,\bxsi]$, using (observed) values from the treatment group, and estimating $\hat{\alpha}$.
Then let
\begin{equation}\label{eq:ps}
  \hat{p}_i=f(\bm{\hat{\alpha}}'\bxsit)
\end{equation}
for all subjects in the experiment.

Finally, fit model
\begin{equation}\label{eq:regression}
  Y_i=\beta_0+\beta_1r_i+\beta_2Z_i+\beta_3Z_i\ri+\bm{\gamma}'\bxy_i+\epsilon_i
\end{equation}
to estimate $\bm{\beta}$ and hence principal effects, where $\bm{x}_i$ is a set of covariates predictive of $Y$ within principal strata.
Let $\bm{\beta}=[\beta_0,\beta_1,\beta_2,\beta_3,\bm{\gamma}']'$.

\sloppy
Following \eqref{eq:stacked}, let $\blam(Z_i,S_i,\bxsi,\bxy_i,Y_i;\bm{\alpha},\bm{\beta})=\begin{pmatrix} Z_i\Omega(\bxsi,S_i;\bm{\alpha})\\ \Psi(\bxy_i,\bxsi,Y_i,Z_i,\ri(\bm{\alpha});\bm{\beta})\end{pmatrix}$, the stacked estimating equations of \eqref{eq:psMod} and \eqref{eq:regression}.
Going forward, for the sake of brevety, we will write $\blam_i(\bm{\alpha},\bm{\beta})=\blam(Z_i,S_i,\bxsi,\bxy_i,Y_i;\bm{\hat{\alpha}},\bm{\hat{\beta}})$, where dependence on the data for $i$ is captured in the subscript $i$, with similar meanings for $\Psi_i(\bm{\alpha},\bm{\beta})$ and $\omega_i(\bm{\alpha})$. 
%Then let $\blamh_i=\blam_i(Z_i,S_i,\bxsi,\bxy_i,Y_i;\bm{\hat{\alpha}},\bm{\hat{\beta}})$ be the estimating equations evaluated at the estimated parameters, and let 
%\begin{equation*}
%\dblam_i=\frac{\partial}{\partial [\alpha,\beta]'} \blam_i\Bigr|_{\substack{\bm{\alpha}=\bm{\hat{\alpha}}\\\bm{\beta}=\bm{\hat{\beta}}}}
%\end{equation*}
%the derivative matrix of $\blam$ evaluated at the estimated parameters. 

The variance-covariance matrix for $\bm{\hat{\alpha}}$ and $\bm{\hat{\beta}}$ can be estimated as:
\begin{equation*}
  \widehat{var}\left([\bm{\hat{\alpha}}',\bm{\hat{\beta}}']'\right)=A^{-1}BA^{-t}
\end{equation*}
where
\begin{equation*}%\label{eq:Amat}
  A=\sum_i \frac{\partial}{\partial [\bm{\alpha},\bm{\beta}]'} \blam_i\Bigr|_{\substack{\bm{\alpha}=\bm{\hat{\alpha}}\\\bm{\beta}=\bm{\hat{\beta}}}}
\end{equation*}
and
\begin{equation*}%\label{eq:Bmat}
  B=\sum_i \blam_i(\bm{\hat{\alpha}},\bm{\hat{\beta}})\blam_i(\bm{\hat{\alpha}},\bm{\hat{\beta}})'
\end{equation*}

Following \citetsupp[][p. 373]{carroll2006measurement}, we %separate the estimating equations $\Psi$ into $\phi$ and $\psi$, estimating equations for models \eqref{
can decompose the matrices into diagonal elements 
\begin{equation*}
    \begin{split}
        A_{1,1}&=\sum_i \partial \Omega_i/\partial \bm{\alpha}|_{\bm{\alpha}=\bm{\hat{\alpha}}}\\
        A_{2,2}&=\sum_i\partial\Psi_i/\partial \bm{\beta}|_{\bm{\beta}=\bm{\hat{\beta}}}\\
        B_{1,1}&=\sum_i\Omega_i(\bm{\hat\alpha})\Omega_i(\bm{\hat\alpha})'\\
        B_{2,2}&=\sum_i \Psi_i(\bm{\hat\alpha},\bm{\hat\beta})\Psi_i(\bm{\hat\alpha},\bm{\hat\beta})'
    \end{split}
\end{equation*}
 that pertain to the parameter sets $\bm{\alpha}$ and $\bm{\beta}$ and the estimating equations for models \eqref{eq:psMod} and  \eqref{eq:regression}, respectively, and 
 \begin{equation*}
     \begin{split}
         A_{21}&=\sum_i\partial\Psi_i/\partial \bm{\alpha}|_{\bm{\alpha}=\bm{\hat{\alpha}}}\\
         B_{12}=B_{21}'&=\sum_i \Omega_i(\bm{\hat{\alpha}})\Psi_i(\bm{\hat{\alpha}},\bm{\hat{\beta}})'
     \end{split}
 \end{equation*}
 which capture to the dependence of model \eqref{eq:regression} on the parameters $\bm{\alpha}$ from \eqref{eq:psMod} and the covariance between the estimating equations of the two models. 
The sub-matrix $A_{12}=\sum_i \partial \Omega_i/\partial \bm{\beta}=0$, since \eqref{eq:psMod} does not depend on $\bm{\beta}$.

The diagonal matrices $A_{1,1}$ and $A_{2,2}$ and $B_{1,1}$ and $B_{2,2}$ are all the typical ``bread'' and ``meat'' matrices from M-estimation of generalized linear models and OLS. 
%In practice, we use the estimates from the \texttt{sandwich} package in \texttt{R}, adjusted in two ways: first, the function \texttt{bread} actually gives $A^{-1}$, not $A$; second, we must pay careful attention to sample sizes, since the sample size for \eqref{eq:psMod} includes only treated observations (with observed $S$) and \eqref{eq:regression} contains all observations.
Calculation of the matrices $B_{12}$ and $B_{21}$ is straightforward after vectors $\Omega_i(\bm{\hat{\alpha}})$ and $\Psi_i(\bm{\hat{\alpha}},\bm{\hat{\beta}})$ have been calculated.
Some specialized calculation is necessary for matrix $A_{21}$. 

\subsection{$A_{21}$ Matrix}
The estimating equations for the regression \eqref{eq:regression} are
\begin{equation}\label{eq:eeOLS}
  \psi(Y_i,\bxy_i,\bxsi,\bm{\beta},\alpha)=X_iY_i-X_iX_i'\bm{\beta}
\end{equation}
Where $X_i=[1,r_i,Z_i,r_iZ_i,\bxyt_i]'$.
In other words,
\begin{align*}
  \psi(Y_i,&\bxy_i,\bxsi,\bm{\beta},\bm{\alpha})=\\
  &\left\{
  \begin{array}{l}
    Y_i-\left(\beta_0+\beta_1r_i+\beta_2Z_i+\beta_3Z_ir_i+\bm{\gamma}'\bxy_i\right)\\
    r_iY_i-r_i\left(\beta_0+\beta_2Z_i+\bm{\gamma}'\bxy_i\right)-r_i^2\left(\beta_1+\beta_3Z_i\right)\\
    Z_iY_i-Z_i\left(\beta_0+\beta_1r_i+\beta_2Z_i+\beta_3Z_ir_i+\bm{\gamma}'\bxy_i\right)\\
    Z_ir_iY_i-Z_ir_i\left(\beta_0+\beta_2Z_i+\bm{\gamma}'\bxy_i\right)-Z_ir_i^2\left(\beta_1+\beta_3Z_i\right)\\
    \bxy_iY_i-\bxy_i\left(\beta_0+\beta_1r_i+\beta_2Z_i+\beta_3Z_ir_i+\bm{\gamma}'\bxy_i\right)
  \end{array}
  \right\}
\end{align*}
(noting that $Z^2=Z$).
These depend on $\bm{\alpha}$ when $r_i=p_i$, i.e. when $Z_i=0$.% or $S_i$ is missing.
Then note that, following \eqref{eq:ps}, and letting $\eta_i=\bm{\alpha}'\bxsit$
\begin{equation}\label{eq:derivP}
  \frac{\partial p_i}{\partial \bm{\alpha}'}=f'(\eta_i)\frac{\partial \eta_i}{\partial \bm{\alpha}'}=f'(\eta_i)\bxsitp
\end{equation}
and that
\begin{equation}
  \frac{\partial p_i^2}{\partial \bm{\alpha}'}=2p_i\frac{\partial p_i}{\partial \bm{\alpha}'}=2f(\eta)f'(\eta_i)\bxsitp=2p_if'(\eta_i)\bxsitp
\end{equation}

Then if $r_i=p_i$,
% \begin{align*}
%   \frac{\partial}{\partial \bm{\alpha}'}&\psi(Y_i,\bxy_i,\bm{\beta},\bm{\alpha})=\\
%   &\left[\begin{array}{c}
%           -(\beta_1+\beta_3Z_i)\frac{\partial p_i}{\partial \bm{\alpha}'}\\
%           (Y_i-\beta_0-\beta_2Z_i-\bm{\gamma}'\bm{x}_{i})\partial p/\partial\bm{\alpha}'-(\beta_1+\beta_3Z_i)\partial p^2/\partial \bm{\alpha}'\\
%           -(\beta_1+\beta_3Z_i)\frac{\partial p_i}{\partial \bm{\alpha}'}Z_i\\
%           Z_i\left[(Y_i-\beta_0-\beta_2Z_i-\bm{\gamma}'\bm{x}_{i})\partial p/\partial\bm{\alpha}'-(\beta_1+\beta_3Z_i)\partial p^2/\partial \bm{\alpha}'\right]\\
%           -(\beta_1+\beta_3Z_i)\frac{\partial p_i}{\partial \bm{\alpha}'}\bxy_i
%         \end{array}\right]\\
%   =& \left[\begin{array}{c}
%           -(\beta_1+\beta_3Z_i)f'(\eta_i)\bxsitp\\
%           \left[Y_i-\beta_0-\beta_2Z_i-\bxy_i'\bm{\gamma}-2p_i(\beta_1+\beta_3Z_i)\right]f'(\eta_i)\bxsitp\\
%           -Z_i\beta_1f'(\eta_i)\bxsitp\\
%           Z_i\left[Y_i-\beta_0-\beta_2Z_i-\bxy_i'\bm{\gamma}-2p_i(\beta_1+\beta_3)\right]f'(\eta_i)\bxsitp\\
%           -(\beta_1+\beta_3Z_i)f'(\eta_i)\bxy_i\bxsitp
%     \end{array}\right]
% \end{align*}

% When $S$ is observed for all members of the treatment group, $Z_i=0$ whenever $r_i=p_i$, so the latter expression reduces to
\begin{align*}
  \frac{\partial}{\partial \bm{\alpha}'}&\psi(Y_i,\bxy_i,\bm{\beta},\bm{\alpha})=\\
  & \left[\begin{array}{c}
          -(\beta_1)f'(\eta_i)\bxsitp\\
          \left[Y_i-X_i'\bm{\beta}-2p_i(\beta_1)\right]f'(\eta_i)\bxsitp\\
          0\\
          0\\
          -\beta_1f'(\eta_i)\bxy_i\bxsitp
    \end{array}\right]
\end{align*}

If $r_i=S_i$, $\frac{\partial}{\partial \bm{\alpha}'}\psi(Y_i,\bxy_i,\bm{\beta},\bm{\alpha})=0$.

\clearpage
\section{Simulation Results}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\textwidth]{simFigs/alphaAUC.pdf}
    \caption{Estimated AUC values from principal score models for each value of $\alpha$ in the simulation, with $n=500$.}
    \label{fig:alphaAUC}
\end{figure}

%\begin{table}
%\caption{Coverage of nominal 95\% confidence intervals}
\input{writeUps/coverageTabAppendix.tex}

%\end{table}

\bibliographystylesupp{plainnat}
\bibliographysupp{MOM}


\end{document}
